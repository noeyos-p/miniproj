import os
# os.environ['OPENCV_AVFOUNDATION_SKIP_AUTH'] = '1'
# Mac ì¹´ë©”ë¼ê¶Œí•œ ë¬¸ì œ ê´€í•´ í™˜ê²½ë³€ìˆ˜ ì¶”ê°€ (Windows ì£¼ì„ì²˜ë¦¬)

import cv2
import torch
import numpy as np
import time
import threading
import queue
import json
import platform
import subprocess

# macOS/Windows í˜¸í™˜ì„±
WIN32COM_AVAILABLE = False
try:
    import win32com.client
    import pythoncom
    WIN32COM_AVAILABLE = True
except ImportError:
    print("Windows TTS ë¼ì´ë¸ŒëŸ¬ë¦¬(win32com)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. macOSì—ì„œëŠ” 'say' ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

from ultralytics import YOLO
from follow_up_service import FollowUpSpeechService

# ==========================================
# FollowUpManager: Handles scheduling and cancellation
# ==========================================
class FollowUpManager:
    def __init__(self, pipeline):
        self.pipeline = pipeline
        self.service = FollowUpSpeechService()
        self.pending_timer = None
        self.lock = threading.Lock()
        self.current_context = None # (label, distance)
        
        # LLM Suppression Logic
        self.llm_call_history = {} # {entity_key: last_call_time}
        self.ALLOWED_CLASSES = {'ì‚¬ëŒ', 'ìë™ì°¨', 'ìì „ê±°'} # person, car, bicycle
        self.MAX_LLM_DIST = 4.0
        self.COOL_DOWN_SEC = 8.0 # 5-8 seconds requirement

    def cancel_pending(self):
        with self.lock:
            if self.pending_timer:
                print("[FollowUpMgr] Cancelling pending follow-up.")
                self.pending_timer.cancel()
                self.pending_timer = None
            self.current_context = None

    def schedule_follow_up(self, label, distance, position_desc, entity_key):
        """Schedules a follow-up with strict suppression gating."""
        # Layer 1: Class and Distance Gating
        if label not in self.ALLOWED_CLASSES:
            # print(f"[FollowUpMgr] LLD Suppressed: {label} is not in whitelist.")
            return

        if distance > self.MAX_LLM_DIST:
            # print(f"[FollowUpMgr] LLM Suppressed: distance {distance:.1f}m > {self.MAX_LLM_DIST}m.")
            return

        # Layer 2: Cool-down Gating
        current_time = time.time()
        last_call = self.llm_call_history.get(entity_key, 0)
        if (current_time - last_call) < self.COOL_DOWN_SEC:
            # print(f"[FollowUpMgr] LLM Suppressed: Cool-down active for {entity_key}.")
            return

        # Passed all gates - proceed to cancel pending and schedule new call
        self.cancel_pending()
        
        with self.lock:
            self.current_context = (label, distance)
            # Record call time to enforce cool-down
            self.llm_call_history[entity_key] = current_time
            
            # Reduced delay since immediate warning is removed
            self.pending_timer = threading.Timer(0.2, self._execute_follow_up, args=(label, distance, position_desc))
            self.pending_timer.start()
            print(f"[FollowUpMgr] Gating Passed. Triggering LLM for {label} at {distance:.1f}m")

    def _generate_rule_based_fallback(self, label, distance, position_desc):
        """Deterministic safety fallback when LLM is unavailable."""
        return f"{position_desc} {distance:.1f}ë¯¸í„°ì— {label}ì´ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”."

    def _execute_follow_up(self, label, distance, position_desc):
        # Verification: Check if the situation is still relevant? 
        # (For this MVP, we rely on the cancellation being called by the loop if object is gone)
        
        # This function runs in a separate thread (Timer thread)
        # It calls the LLM, which is NOT in the detection loop.
        explanation = self.service.generate_explanation(label, distance, position_desc)
        
        # Fallback if LLM fails (explanation is None)
        if explanation is None:
            print("[FollowUpMgr] LLM API failure. Triggering rule-based fallback.")
            explanation = self._generate_rule_based_fallback(label, distance, position_desc)

        if explanation:
            print(f"[FollowUpMgr] Speech Output: {explanation}")
            # Only play if not cancelled during LLM call
            with self.lock:
                if self.current_context == (label, distance):
                    # Play the explanation through the pipeline's TTS worker
                    self.pipeline.speak(explanation, is_follow_up=True)
                else:
                    print("[FollowUpMgr] Context changed during wait/call, discarding result.")

# Whisper ë° PyAudio ì„¤ì •
WHISPER_AVAILABLE = False
PYAUDIO_AVAILABLE = False
try:
    import whisper  # openai-whisper
    import pyaudio
    WHISPER_AVAILABLE = True
    PYAUDIO_AVAILABLE = True
except ImportError:
    pass

# ==========================================
# Optimized MVP Test Pipeline: TTS (ìŒì„± ì•ˆë‚´) ë²„ì „
# ==========================================

class MVPTestPipeline:
    def __init__(self, web_mode=False):
        """
        web_mode=True: ì›¹ ëª¨ë“œ (ì„œë²„ STT/TTS ë¹„í™œì„±í™”, í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì²˜ë¦¬)
        web_mode=False: ë¡œì»¬ ëª¨ë“œ (ì„œë²„ STT/TTS í™œì„±í™”)
        """
        self.web_mode = web_mode
        print("ìŒì„± ì§€ì› ëª¨ë“œë¡œ ì „í™˜ ì¤‘... ëª¨ë¸ ë¡œë”© ì¤‘...")

        # ì„¤ì •ê°’
        self.inference_size = (320, 320)
        self.frame_skip = 3
        self.frame_count = 0
        # self.K_DEPTH = 100.0  # ì‹¤ì¸¡ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (0.5mâ†’1.2m, 1mâ†’2m, 2mâ†’4~7m â†’ ì•½ 2ë°° ë³´ì •)
        # ê±°ë¦¬ë³„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜: ê°€ê¹Œìš´ ê±°ë¦¬ì™€ ë¨¼ ê±°ë¦¬ì— ë‹¤ë¥¸ ë³´ì •ê°’ ì ìš©
        self.K_DEPTH_NEAR = 40.0   # 1m ë¯¸ë§Œ (ê°€ê¹Œìš´ ê±°ë¦¬ìš©)
        self.K_DEPTH_FAR = 100.0   # 1m ì´ìƒ (ë¨¼ ê±°ë¦¬ìš©, ê¸°ì¡´ ê°’)
        self.DISTANCE_THRESHOLD = 100.0  # raw_val ê¸°ì¤€: ì´ ê°’ë³´ë‹¤ í¬ë©´ ê°€ê¹Œìš´ ê±°ë¦¬
        self.running = False  # ì œì–´ìš© í”Œë˜ê·¸

        # ìŒì„± ì•ˆë‚´ ì„¤ì • (ë³¼ë¥¨ ë° ë®¤íŠ¸)
        self.volume = 100  # 0 ~ 100
        self.is_muted = False

        # TTS í ë° ìŠ¤ë ˆë“œ ì´ˆê¸°í™” (ì›¹ ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ)
        self.speech_queue = queue.Queue()
        if not web_mode:
            self.tts_thread = threading.Thread(target=self._tts_worker, daemon=True)
            self.tts_thread.start()

        # STT (ìŒì„± ì¸ì‹) ì´ˆê¸°í™” (ì›¹ ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ)
        self.stt_thread = None
        if not web_mode and WHISPER_AVAILABLE and PYAUDIO_AVAILABLE:
            try:
                # Whisper ëª¨ë¸ ë¡œë”©
                print("Whisper 'tiny' ëª¨ë¸ ë¡œë”© ì¤‘...")
                # self.whisper_model = whisper.load_model("base")
                self.whisper_model = whisper.load_model("tiny")  # ë¹ ë¥¸ ì²˜ë¦¬
                self.stt_thread = threading.Thread(target=self._stt_worker, daemon=True)
                self.stt_thread.start()
            except Exception as e:
                print(f"STT ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        elif not web_mode:
            print(f"ìŒì„± ëª…ë ¹ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜)")
        
        # FollowUp Manager ì´ˆê¸°í™”
        self.follow_up_mgr = FollowUpManager(self)

        # ì‹œì‘ ì•Œë¦¼ (ìŠ¤í”¼ì»¤ í™•ì¸ìš©) - ì›¹ ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ
        if not web_mode:
            self.speak("ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        # ìŒì„± ìƒíƒœ ê´€ë¦¬
        self.announced_objects = {} # {label: last_seen_time}
        self.announce_timeout = 8.0 # 8ì´ˆ ë™ì•ˆ ì•ˆ ë³´ì´ë©´ ì•ˆë‚´ ëª©ë¡ì—ì„œ ì‚­ì œ (ë‹¤ì‹œ ë‚˜íƒ€ë‚˜ë©´ ë§í•¨)

        # ë¼ë²¨ë³„ ì¿¨ë‹¤ìš´ (ê°™ì€ ì¢…ë¥˜ ê°ì²´ ë°˜ë³µ ì•ˆë‚´ ë°©ì§€)
        self.label_cooldown = {}  # {label: last_announce_time}
        self.cooldown_time = 5.0  # 5ì´ˆ ì¿¨ë‹¤ìš´

        # Track IDë³„ ì¿¨ë‹¤ìš´ (ê°™ì€ ê°ì²´ ë°˜ë³µ ì•ˆë‚´ ë°©ì§€)
        self.track_id_cooldown = {}  # {track_id: last_announce_time}
        self.track_cooldown_time = 15.0  # ê°™ì€ ê°ì²´ëŠ” 15ì´ˆ ë™ì•ˆ ë°˜ë³µ ì•ˆë‚´ ì•ˆ í•¨

        # ì›¹ ëª¨ë“œìš© LLM ì‘ë‹µ ìºì‹œ
        self.web_speech_cache = {}  # {entity_key: llm_generated_text}
        self.cache_lock = threading.Lock()

        # GPU ë””ë°”ì´ìŠ¤ ì„ íƒ: CUDA (NVIDIA) > MPS (Apple Silicon) > CPU
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
            self.yolo_device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = torch.device("mps")
            self.yolo_device = "mps"
        else:
            self.device = torch.device("cpu")
            self.yolo_device = "cpu"

        print(f"ğŸš€ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")

        # ëª¨ë¸ ë¡œë”©
        # YOLOv8 Nano ëª¨ë¸ ë¡œë“œ (GPU ì‚¬ìš©)
        self.yolo_model = YOLO('yolov8n.pt')
        self.yolo_model.to(self.yolo_device)
        print(f"âœ… YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.yolo_device}")

        # Depth-Anything-V2-Small ëª¨ë¸ ë¡œë“œ
        from transformers import pipeline as hf_pipeline
        from PIL import Image as PILImage
        self.depth_model_type = "Depth-Anything-V2-Small"
        print(f"Depth ëª¨ë¸ ë¡œë”© ì¤‘: {self.depth_model_type} on {self.device}")
        self.depth_pipe = hf_pipeline(task="depth-estimation", model="depth-anything/Depth-Anything-V2-Small-hf", device=self.device)

        self.last_objects = []
        self.last_depth_map = None
        self.last_depth_viz = None
        
        # ì›¹ ìŠ¤íŠ¸ë¦¬ë°ìš© ë²„í¼
        self.last_web_frame = None
        self.frame_lock = threading.Lock()

        # í•œêµ­ì–´ í´ë˜ìŠ¤ ë§µ
        self.class_names_ko = {
            'person': 'ì‚¬ëŒ', 'bicycle': 'ìì „ê±°', 'car': 'ìë™ì°¨', 'motorcycle': 'ì˜¤í† ë°”ì´',
            'bus': 'ë²„ìŠ¤', 'truck': 'íŠ¸ëŸ­', 'traffic light': 'ì‹ í˜¸ë“±', 'stop sign': 'ì •ì§€ í‘œì§€íŒ',
            'bench': 'ë²¤ì¹˜', 'dog': 'ê°œ', 'cat': 'ê³ ì–‘ì´', 'backpack': 'ë°°ë‚­', 'umbrella': 'ìš°ì‚°',
            'handbag': 'í•¸ë“œë°±', 'tie': 'ë„¥íƒ€ì´', 'suitcase': 'ì—¬í–‰ê°€ë°©', 'sports ball': 'ê³µ',
            'bottle': 'ë³‘', 'wine glass': 'ì™€ì¸ì”', 'cup': 'ì»µ', 'fork': 'í¬í¬', 'knife': 'ì¹¼',
            'spoon': 'ìˆŸê°€ë½', 'bowl': 'ê·¸ë¦‡', 'banana': 'ë°”ë‚˜ë‚˜', 'apple': 'ì‚¬ê³¼', 'sandwich': 'ìƒŒë“œìœ„ì¹˜',
            'orange': 'ì˜¤ë Œì§€', 'broccoli': 'ë¸Œë¡œì½œë¦¬', 'carrot': 'ë‹¹ê·¼', 'hot dog': 'í•«ë„ê·¸', 'pizza': 'í”¼ì',
            'donut': 'ë„ë„›', 'cake': 'ì¼€ì´í¬', 'chair': 'ì˜ì', 'couch': 'ì†ŒíŒŒ', 'potted plant': 'í™”ë¶„',
            'bed': 'ì¹¨ëŒ€', 'dining table': 'ì‹íƒ', 'toilet': 'ë³€ê¸°', 'tv': 'TV', 'laptop': 'ë…¸íŠ¸ë¶',
            'mouse': 'ë§ˆìš°ìŠ¤', 'remote': 'ë¦¬ëª¨ì»¨', 'keyboard': 'í‚¤ë³´ë“œ', 'cell phone': 'í•¸ë“œí°',
            'microwave': 'ì „ìë ˆì¸ì§€', 'oven': 'ì˜¤ë¸', 'í† ìŠ¤í„°': 'í† ìŠ¤í„°', 'sink': 'ì‹±í¬ëŒ€',
            'refrigerator': 'ëƒ‰ì¥ê³ ', 'book': 'ì±…', 'clock': 'ì‹œê³„', 'vase': 'ê½ƒë³‘', 'scissors': 'ê°€ìœ„',
            'teddy bear': 'ê³°ì¸í˜•', 'hair drier': 'í—¤ì–´ë“œë¼ì´ì–´', 'toothbrush': 'ì¹«ì†”'
        }

        # Walking assistance ROI (Center 40%)
        # self.roi_x_min = 0.3
        # self.roi_x_max = 0.7
        # ë” ë§ì€ ê°ì²´ ê°ì§€ë¥¼ ìœ„í•´ ROIë¥¼ ì¤‘ì•™ 80%ë¡œ í™•ì¥
        self.roi_x_min = 0.1
        self.roi_x_max = 0.9

        # Spatial Bucketing for entity differentiation
        self.DIST_BIN_SIZE = 1.5   # meters
        self.POS_BIN_SIZE = 0.1    # 10% of frame width
        
        # Velocity Tracking for Approach Detection
        self.entity_velocity_history = {} # {entity_key: (last_dist, last_time)}
        self.APPROACH_THRESHOLD_SPEED = 1.5 # m/s (approx 5.4 km/h)
        self.APPROACH_THRESHOLD_DIST = 4.0  # meters

    def _tts_worker(self):
        """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ TTS ì•ˆë‚´ë¥¼ ì²˜ë¦¬"""
        # Windowsìš©
        if WIN32COM_AVAILABLE:
            pythoncom.CoInitialize()
            speaker = win32com.client.Dispatch("SAPI.SpVoice")
            current_process = None
        else:
            # macOSìš©: 'say' ëª…ë ¹ì–´ ì‚¬ìš©
            speaker = None
            current_process = None

        while True:
            # íì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´ (í…ìŠ¤íŠ¸, ê°•ì œì¤‘ì§€ì—¬ë¶€, follow_upì—¬ë¶€)
            item = self.speech_queue.get()
            if item is None: break

            text, force_stop, is_follow_up = item

            # ë®¤íŠ¸ ìƒíƒœë©´ ë¬´ì‹œ (ë‹¨, ê°•ì œ ì¢…ë£Œ ì•ˆë‚´ëŠ” ì˜ˆì™¸)
            if self.is_muted and not force_stop:
                self.speech_queue.task_done()
                continue

            # force_stopì´ Trueì´ë©´ í˜„ì¬ ë§í•˜ê³  ìˆëŠ” ê²ƒì„ ì¤‘ì§€
            if force_stop and current_process and current_process.poll() is None:
                current_process.terminate()
                current_process = None

            print(f"[TTS ë°œí™” ì‹œì‘] {text} (ê°•ì œì¢…ë£Œ: {force_stop}, í›„ì†: {is_follow_up})")
            try:
                if WIN32COM_AVAILABLE:
                    # Windowsìš©: win32com ì‚¬ìš©
                    speaker.Volume = self.volume
                    flags = 2 if force_stop else 0
                    speaker.Speak(text, flags)
                else:
                    # macOSìš©: say ëª…ë ¹ì–´ ì‚¬ìš©
                    current_process = subprocess.Popen(
                        ['say', '-v', 'Yuna', '-r', '200', text],
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL
                    )
                    current_process.wait()

            except Exception as e:
                print(f"[TTS ì˜¤ë¥˜] {e}")
            print(f"[TTS ë°œí™” ì™„ë£Œ] {text}")

            self.speech_queue.task_done()

    def _stt_worker(self):
        """ë§ˆì´í¬ ì†Œë¦¬ë¥¼ ë“£ê³  ëª…ë ¹ì–´ë¥¼ ì¸ì‹í•˜ëŠ” ìŠ¤ë ˆë“œ (ì—”í„° í‚¤ ë°©ì‹)"""
        # Whisper
        import tempfile
        import wave

        p = pyaudio.PyAudio()
        stream = p.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=1024)

        print("ğŸ™ï¸ ìŒì„± ì¸ì‹ ì¤€ë¹„ ì™„ë£Œ.")
        print("ğŸ“¢ [ì—”í„°]ë¥¼ ëˆ„ë¥´ë©´ 3ì´ˆê°„ ë…¹ìŒì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        while True:
            # ì—”í„° ì…ë ¥ ëŒ€ê¸°
            input("ğŸ¤ ë…¹ìŒí•˜ë ¤ë©´ [ì—”í„°]ë¥¼ ëˆ„ë¥´ì„¸ìš”...")

            print("ğŸ”´ ë…¹ìŒ ì¤‘... (3ì´ˆ)")

            # 3ì´ˆê°„ ë…¹ìŒ
            frames = []
            for _ in range(0, int(16000 / 1024 * 3)):
                data = stream.read(1024, exception_on_overflow=False)
                frames.append(data)

            print("âŒ› ì¸ì‹ ì¤‘...")

            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                wf = wave.open(f.name, 'wb')
                wf.setnchannels(1)
                wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))
                wf.setframerate(16000)
                wf.writeframes(b''.join(frames))
                wf.close()

                # Whisperë¡œ ì¸ì‹
                # fp16=False: CPUì—ì„œ FP16 ë¯¸ì§€ì› ê²½ê³  ì œê±°
                result = self.whisper_model.transcribe(f.name, language="ko", fp16=False)
                text = result["text"].replace(" ", "")

                os.unlink(f.name)

            if not text:
                print("âŒ ì¸ì‹ëœ ìŒì„±ì´ ì—†ìŠµë‹ˆë‹¤.")
                continue

            print(f"âœ… ìŒì„± ì¸ì‹ ê²°ê³¼: {text}")
            self.handle_command(text)

    def handle_command(self, text):
        """ìŒì„± ì¸ì‹ì„ í†µí•´ ë“¤ì–´ì˜¨ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ëª…ë ¹ ìˆ˜í–‰"""
        # ëª…ë ¹ì–´ íŒë³„ (ê³µë°± ì œê±° í›„ ë¹„êµ)
        text = text.replace(" ", "")
        response_text = None  # ì›¹ ëª¨ë“œìš© ì‘ë‹µ í…ìŠ¤íŠ¸

        if "ì¢…ë£Œ" in text or "ì¢…ë£Œí•´" in text or "ì¢…ë£Œí•´ì¤˜" in text or "ì‹œìŠ¤í…œ ì¢…ë£Œ" in text:
            response_text = "ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤."
            self.speak(response_text, force_stop=True)
            self.running = False
        elif "ì‹œì‘" in text or "ì‹œì‘í•´ì¤˜" in text or "ì‹¤í–‰" in text or "ì‹¤í–‰í•´ì¤˜" in text:
            response_text = "ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤."
            self.speak(response_text, force_stop=True)
        elif "ì¡°ìš©íˆí•´" in text or "ì •ì§€í•´" in text or "ì¤‘ì§€í•´" in text or "ìŒì†Œê±°" in text or "ìŒì†Œê±°í•´ì¤˜" in text:
            self.is_muted = True
            response_text = "ìŒì„± ì•ˆë‚´ë¥¼ ì¼ì‹œ ì •ì§€í•©ë‹ˆë‹¤."
            self.speak(response_text, force_stop=True)
        elif "ë§í•´ì¤˜" in text or "ë‹¤ì‹œë§í•´" in text or "ë‹¤ì‹œë§í•´ì¤˜" in text or "ìŒì„±ì•ˆë‚´ì‹œì‘" in text or "ìŒì„±ì•ˆë‚´í•´ì¤˜" in text:
            self.is_muted = False
            response_text = "ìŒì„± ì•ˆë‚´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."
            self.speak(response_text)

        return response_text  # ì›¹ ëª¨ë“œì—ì„œ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ë‹¬

    def speak(self, text, force_stop=False, is_follow_up=False):
        """ì•ˆë‚´ ë¬¸êµ¬ë¥¼ íì— ì¶”ê°€ (ë¹„ë™ê¸°)"""
        # ë³€ê²½: ì›¹ ëª¨ë“œì¼ ë•ŒëŠ” ì„œë²„ì—ì„œ TTSë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ (í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì²˜ë¦¬)
        if self.web_mode:
            return

        if force_stop:
            # ê¸°ì¡´ íì— ìŒ“ì¸ ëª¨ë“  ë©”ì‹œì§€ ë¬´ì‹œí•˜ë„ë¡ í ë¹„ìš°ê¸° ì‹œë„
            while not self.speech_queue.empty():
                try:
                    self.speech_queue.get_nowait()
                    self.speech_queue.task_done()
                except:
                    break
        self.speech_queue.put((text, force_stop, is_follow_up))

    def stage2_yolo_optimized(self, frame):
        # ByteTrack ì¶”ì  ì‚¬ìš©
        # results = self.yolo_model.track(frame, imgsz=320, verbose=False, conf=0.25, tracker="bytetrack.yaml", persist=True)
        results = self.yolo_model.track(frame, imgsz=640, verbose=False, conf=0.20, tracker="bytetrack.yaml", persist=True)  # ê°€ê¹Œìš´ ê°ì²´ ì¸ì‹ ê°œì„ : í•´ìƒë„ 640, confidence 0.20
        objects = []

        h, w = frame.shape[:2] if len(frame.shape) == 3 else (frame.shape[0], frame.shape[1])

        for r in results:
            boxes = r.boxes
            for box in boxes:
                conf = float(box.conf[0])
                b = box.xyxy[0].cpu().numpy().astype(int)
                cls_id = int(box.cls[0])
                model_label = self.yolo_model.names[cls_id]
                ko_label = self.class_names_ko.get(model_label, model_label)

                # Track ID ì¶”ì¶œ (ByteTrackì—ì„œ ì œê³µ)
                track_id = None
                if box.id is not None:
                    track_id = int(box.id[0])

                # ë°•ìŠ¤ í¬ê¸° ê³„ì‚° (í™”ë©´ ëŒ€ë¹„ ë¹„ìœ¨)
                box_width = b[2] - b[0]
                box_height = b[3] - b[1]
                box_area_ratio = (box_width * box_height) / (w * h)

                # ë””ë²„ê¹…: ëª¨ë“  ê°ì§€ëœ ê°ì²´ ë¡œê·¸
                print(f"[YOLO Debug] {ko_label} (conf:{conf:.2f}, size:{box_area_ratio*100:.1f}%, ID:{track_id})")

                objects.append({
                    'box': b,
                    'label': ko_label,
                    'track_id': track_id,
                    'confidence': conf
                })
        return objects

    def stage3_depth_optimized(self, frame):

        # Depth-Anything-V2-Small ë°©ì‹
        from PIL import Image as PILImage

        # BGR -> RGB ë³€í™˜ í›„ PIL Imageë¡œ ë³€í™˜
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = PILImage.fromarray(rgb_frame)

        # Depth estimation
        result = self.depth_pipe(pil_image)
        depth_map = np.array(result["depth"])

        # ì›ë³¸ í”„ë ˆì„ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        if depth_map.shape[:2] != frame.shape[:2]:
            depth_map = cv2.resize(depth_map, (frame.shape[1], frame.shape[0]))

        # ì‹œê°í™”ìš© depth map ìƒì„±
        depth_min, depth_max = depth_map.min(), depth_map.max()
        depth_norm = (255 * (depth_map - depth_min) / (depth_max - depth_min + 1e-5)).astype(np.uint8)
        depth_color = cv2.applyColorMap(depth_norm, cv2.COLORMAP_MAGMA)

        return depth_map, depth_color

    def raw_to_meters(self, raw_val):
        if raw_val <= 0: return float('inf')

        # êµ¬ê°„ë³„ ë³´ì •: raw_valì´ í¬ë©´ ê°€ê¹Œìš´ ê±°ë¦¬ (K_DEPTH_NEAR ì‚¬ìš©)
        if raw_val > self.DISTANCE_THRESHOLD:
            meters = self.K_DEPTH_NEAR / (raw_val + 1e-5)
        else:
            meters = self.K_DEPTH_FAR / (raw_val + 1e-5)

        return meters
    
    # ëª¨ë°”ì¼ ì¹´ë©”ë¼ ë° ìŒì„±
    def _generate_web_llm_async(self, entity_key, label, meters, pos_desc):
        """ì›¹ ëª¨ë“œìš© ë¹„ë™ê¸° LLM ìƒì„± (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        try:
            # LLMìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ ìƒì„±
            explanation = self.follow_up_mgr.service.generate_explanation(label, meters, pos_desc)

            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë¬¸ì¥
            if not explanation:
                explanation = f"{pos_desc} {meters:.1f}ë¯¸í„°ì— {label}ì´ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”."

            # ìºì‹œì— ì €ì¥
            with self.cache_lock:
                self.web_speech_cache[entity_key] = explanation
                print(f"[Web LLM] ìºì‹œ ì €ì¥: {entity_key} -> {explanation}")
        except Exception as e:
            print(f"[Web LLM] ì˜¤ë¥˜: {e}")

    def process_web_frame(self, frame):
        """ì›¹ì—ì„œ ì „ì†¡ëœ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ ì´ë¯¸ì§€ ë°˜í™˜"""
        processed, _ = self.process_web_frame_with_speech(frame)
        return processed

    def process_web_frame_with_speech(self, frame):
        """ì›¹ì—ì„œ ì „ì†¡ëœ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ ì´ë¯¸ì§€ì™€ ìŒì„± í…ìŠ¤íŠ¸ ë°˜í™˜"""
        self.frame_count += 1
        current_time = time.time()
        speech_text = None

        # YOLO ë° ê¹Šì´ ì¶”ì •
        if self.frame_count % self.frame_skip == 1 or self.last_depth_map is None:
            self.last_objects = self.stage2_yolo_optimized(frame)
            self.last_depth_map, self.last_depth_viz = self.stage3_depth_optimized(frame)

        display_frame = frame.copy()
        h, w = frame.shape[:2]
        roi_left = int(w * self.roi_x_min)
        roi_right = int(w * self.roi_x_max)

        # ROI ê°€ì´ë“œ ë¼ì¸
        cv2.line(display_frame, (roi_left, 0), (roi_left, h), (0, 0, 255), 2)
        cv2.line(display_frame, (roi_right, 0), (roi_right, h), (0, 0, 255), 2)

        # ROI ë‚´ì˜ ëª¨ë“  ê°ì§€ëœ ê°ì²´ ìˆ˜ì§‘ (ê±°ë¦¬ 10m ì´ë‚´)
        detected_objects = []

        for obj in self.last_objects:
            b = obj['box']
            cx = (b[0] + b[2]) // 2
            cy = int(b[3] * 0.9)
            if roi_left <= cx <= roi_right:
                h_d, w_d = self.last_depth_map.shape
                cx_d, cy_d = max(0, min(cx, w_d-1)), max(0, min(cy, h_d-1))
                raw_val = self.last_depth_map[cy_d, cx_d]
                meters = self.raw_to_meters(raw_val)

                # 10m ì´ë‚´ì˜ ê°ì²´ë§Œ ì¶”ê°€
                if meters < 10.0:
                    detected_objects.append({
                        'label': obj['label'],
                        'box': b,
                        'meters': meters,
                        'cx': cx,
                        'track_id': obj.get('track_id'),
                        'confidence': obj.get('confidence', 0.0)
                    })

        # ê±°ë¦¬ ìˆœìœ¼ë¡œ ì •ë ¬ (ê°€ê¹Œìš´ ìˆœ)
        detected_objects.sort(key=lambda x: x['meters'])

        # ê°ì§€ëœ ê°ì²´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (í´ë¼ì´ì–¸íŠ¸ ë Œë”ë§ìš©)
        detection_data = []
        current_entities = set()

        # ëª¨ë“  ê°ì§€ëœ ê°ì²´ ì²˜ë¦¬
        for detected_obj in detected_objects:
            b = detected_obj['box']
            label_name = detected_obj['label']
            meters = detected_obj['meters']
            track_id = detected_obj.get('track_id')

            dist_bin = int(meters / self.DIST_BIN_SIZE)
            pos_bin = int((detected_obj['cx'] / w) / self.POS_BIN_SIZE)
            entity_key = (label_name, dist_bin, pos_bin)
            current_entities.add(entity_key)

            # í´ë¼ì´ì–¸íŠ¸ ë Œë”ë§ìš© ë°ì´í„° ì¶”ê°€
            detection_data.append({
                'box': [int(b[0]), int(b[1]), int(b[2]), int(b[3])],
                'label': str(label_name),
                'distance': float(round(meters, 1)),
                'roi': [int(roi_left), int(roi_right)],
                'track_id': track_id,
                'confidence': float(round(detected_obj.get('confidence', 0.0), 2))
            })

            # ì„œë²„ ì‹œê°í™” (ë…¸íŠ¸ë¶ ëª¨ë“œìš©)
            cv2.rectangle(display_frame, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 3)
            # Track ID í‘œì‹œ ì¶”ê°€
            label_text = f"{label_name} {meters:.1f}m"
            if track_id is not None:
                label_text = f"ID:{track_id} {label_name} {meters:.1f}m"
            cv2.putText(display_frame, label_text, (b[0], b[1]-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

        # ìŒì„± ì•ˆë‚´ëŠ” ê°€ì¥ ê°€ê¹Œìš´ ê°ì²´ë§Œ (ì²« ë²ˆì§¸ ê°ì²´)
        if detected_objects:
            closest_obj = detected_objects[0]
            label_name = closest_obj['label']
            meters = closest_obj['meters']
            track_id = closest_obj.get('track_id')

            dist_bin = int(meters / self.DIST_BIN_SIZE)
            pos_bin = int((closest_obj['cx'] / w) / self.POS_BIN_SIZE)
            entity_key = (label_name, dist_bin, pos_bin)

            # --- Rapid Approach Detection (Independent of Cooldown) ---
            if entity_key in self.entity_velocity_history:
                last_dist, last_time = self.entity_velocity_history[entity_key]
                dt = current_time - last_time
                if dt > 0:
                    approach_speed = (last_dist - meters) / dt # m/s
                    if meters < self.APPROACH_THRESHOLD_DIST and approach_speed >= self.APPROACH_THRESHOLD_SPEED:
                        warning_msg = f"ìœ„í—˜! {label_name}ì´ ë§¤ìš° ë¹ ë¥´ê²Œ ì ‘ê·¼ ì¤‘ì…ë‹ˆë‹¤."
                        print(f"[RapidApproach] Speed: {approach_speed:.2f} m/s, Dist: {meters:.1f}m. Triggering Warning.")
                        if self.web_mode:
                            speech_text = warning_msg
                        else:
                            self.speak(warning_msg, force_stop=True)

            # Update velocity history every frame for tracking
            self.entity_velocity_history[entity_key] = (meters, current_time)

            # ìŒì„± ì•ˆë‚´ (Track ID ì¿¨ë‹¤ìš´ ìš°ì„ , ë¼ë²¨ ì¿¨ë‹¤ìš´ ì²´í¬, ë®¤íŠ¸ ìƒíƒœ ì²´í¬)
            can_announce = True

            # Track IDê°€ ìˆìœ¼ë©´ Track ID ì¿¨ë‹¤ìš´ ì²´í¬ (ê°™ì€ ê°ì²´ëŠ” 15ì´ˆ ë™ì•ˆ ë°˜ë³µ ì•ˆ í•¨)
            if track_id is not None and track_id in self.track_id_cooldown:
                if (current_time - self.track_id_cooldown[track_id]) < self.track_cooldown_time:
                    can_announce = False
            # Track IDê°€ ì—†ê±°ë‚˜ ì¿¨ë‹¤ìš´ì´ ì•„ë‹ˆë©´ ë¼ë²¨ ì¿¨ë‹¤ìš´ ì²´í¬
            elif label_name in self.label_cooldown:
                if (current_time - self.label_cooldown[label_name]) < self.cooldown_time:
                    can_announce = False

            # LLM ìƒì„± ì‹œì‘
            if can_announce and not self.is_muted:
                pos_desc = "ì •ë©´"
                if closest_obj['cx'] < roi_left + (roi_right - roi_left) * 0.3:
                    pos_desc = "ì™¼ìª½"
                elif closest_obj['cx'] > roi_left + (roi_right - roi_left) * 0.7:
                    pos_desc = "ì˜¤ë¥¸ìª½"

                # ë¹„ë™ê¸° LLM ìƒì„± ì‹œì‘
                llm_thread = threading.Thread(
                    target=self._generate_web_llm_async,
                    args=(entity_key, label_name, meters, pos_desc),
                    daemon=True
                )
                llm_thread.start()

            # Track ID ì¿¨ë‹¤ìš´ì„ í™œìš©í•˜ì—¬ ì¤‘ë³µ ì¬ìƒ ë°©ì§€
            can_play = True

            # Track IDê°€ ìˆìœ¼ë©´ Track ID ì¿¨ë‹¤ìš´ ì²´í¬
            if track_id is not None and track_id in self.track_id_cooldown:
                if (current_time - self.track_id_cooldown[track_id]) < self.track_cooldown_time:
                    can_play = False
            # Track IDê°€ ì—†ê±°ë‚˜ ì¿¨ë‹¤ìš´ì´ ì•„ë‹ˆë©´ ë¼ë²¨ ì¿¨ë‹¤ìš´ ì²´í¬
            elif label_name in self.label_cooldown:
                if (current_time - self.label_cooldown[label_name]) < self.cooldown_time:
                    can_play = False

            if not self.is_muted and can_play:
                with self.cache_lock:
                    if entity_key in self.web_speech_cache:
                        speech_text = self.web_speech_cache[entity_key]
                        self.announced_objects[entity_key] = current_time
                        self.label_cooldown[label_name] = current_time
                        # Track ID ì¿¨ë‹¤ìš´ ì—…ë°ì´íŠ¸
                        if track_id is not None:
                            self.track_id_cooldown[track_id] = current_time
                        print(f"[Web TTS] ìºì‹œì—ì„œ ìŒì„± ì¬ìƒ: {speech_text} (ID:{track_id})")
                    else:
                        pos_desc = "ì •ë©´"
                        if closest_obj['cx'] < roi_left + (roi_right - roi_left) * 0.3:
                            pos_desc = "ì™¼ìª½"
                        elif closest_obj['cx'] > roi_left + (roi_right - roi_left) * 0.7:
                            pos_desc = "ì˜¤ë¥¸ìª½"
                        speech_text = f"{pos_desc} {meters:.1f}ë¯¸í„°ì— {label_name}"
                        self.label_cooldown[label_name] = current_time
                        # Track ID ì¿¨ë‹¤ìš´ ì—…ë°ì´íŠ¸
                        if track_id is not None:
                            self.track_id_cooldown[track_id] = current_time
                        print(f"[Web TTS] ê¸°ë³¸ ìŒì„± ì¬ìƒ: {speech_text} (ID:{track_id})")

        # ì˜¤ë˜ëœ ê°ì²´ ì •ë¦¬
        for entity_key in list(self.announced_objects.keys()):
            if entity_key not in current_entities:
                if current_time - self.announced_objects[entity_key] > self.announce_timeout:
                    del self.announced_objects[entity_key]
                    # Also clean up velocity history
                    if entity_key in self.entity_velocity_history:
                        del self.entity_velocity_history[entity_key]
                    # ìºì‹œë„ ì‚­ì œ
                    with self.cache_lock:
                        if entity_key in self.web_speech_cache:
                            del self.web_speech_cache[entity_key]
                            print(f"[Web LLM] ìºì‹œ ì‚­ì œ: {entity_key}")

        return display_frame, speech_text, detection_data

    def run(self, headless=False):
        """
        headless=True: GUI ì—†ì´ ì‹¤í–‰ (Mac ì›¹ ëª¨ë“œìš©)
        headless=False: GUI ì°½ í‘œì‹œ (Windows ë˜ëŠ” ë¡œì»¬ ì‹¤í–‰ìš©)
        """
        # ë¨¼ì € ë‚´ì¥ ì¹´ë©”ë¼(0) ì‹œë„, ì‹¤íŒ¨í•˜ë©´ ì™¸ì¥ ì¹´ë©”ë¼(1) ì‹œë„
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            print("ë‚´ì¥ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì™¸ì¥ ì¹´ë©”ë¼ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
            cap = cv2.VideoCapture(1)
            if not cap.isOpened():
                print("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("Windows ì„¤ì •ì—ì„œ ì¹´ë©”ë¼ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
                print("ì„¤ì • > ê°œì¸ì •ë³´ > ì¹´ë©”ë¼ > ì•±ì´ ì¹´ë©”ë¼ì— ì•¡ì„¸ìŠ¤í•˜ë„ë¡ í—ˆìš©")
                return

        window_name_main = "MVP Test - Color (YOLO)"
        window_name_depth = "MVP Test - Depth (Depth-Anything-V2-Small)"

        # GUI ëª¨ë“œì¼ ë•Œë§Œ ì°½ ìƒì„±
        if not headless:
            cv2.namedWindow(window_name_main)
            cv2.namedWindow(window_name_depth)

        print("\n=== ìŒì„± ì•ˆë‚´(TTS)ê°€ ìµœì í™”ëœ MVP íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")

        # ì‹œì‘ ì‹œ ì•ˆë‚´ ìŒì„± (ì›¹ ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ)
        if not self.web_mode:
            self.speak("ë³´ì¡° ì‹œìŠ¤í…œ ì•ˆë‚´ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", force_stop=True)

        self.running = True
        last_log_time = 0
        log_interval = 6.0 

        while self.running:
            ret, frame = cap.read()
            if not ret: break
            
            self.frame_count += 1
            current_time = time.time()
            
            # --- íŒŒì´í”„ë¼ì¸ ì—°ì‚° ---
            if self.frame_count % self.frame_skip == 1 or self.last_depth_map is None:
                self.last_objects = self.stage2_yolo_optimized(frame)
                self.last_depth_map, self.last_depth_viz = self.stage3_depth_optimized(frame)
            
            display_frame = frame.copy()
            should_log = (current_time - last_log_time) >= log_interval

            # --- ROI í•„í„°ë§ ë° ê°€ì¥ ê°€ê¹Œìš´ ë¬¼ì²´ ì„ íƒ ---
            h, w = frame.shape[:2]
            roi_left = int(w * self.roi_x_min)
            roi_right = int(w * self.roi_x_max)
            
            closest_obj = None
            min_meters = float('inf')

            for obj in self.last_objects:
                b = obj['box']
                cx = (b[0] + b[2]) // 2
                cy = int(b[3] * 0.9)
                
                # ROI ë‚´ë¶€ì— ì¤‘ì‹¬ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                if roi_left <= cx <= roi_right:
                    h_d, w_d = self.last_depth_map.shape
                    cx_d, cy_d = max(0, min(cx, w_d-1)), max(0, min(cy, h_d-1))
                    
                    raw_val = self.last_depth_map[cy_d, cx_d]
                    meters = self.raw_to_meters(raw_val)
                    
                    # ê°€ì¥ ê°€ê¹Œìš´ ë¬¼ì²´ ê°±ì‹ 
                    if meters < min_meters:
                        min_meters = meters
                        closest_obj = {
                            'label': obj['label'],
                            'box': b,
                            'meters': meters,
                            'cx': cx
                        }

            # --- ì‹œê°í™” ë° ì•ˆë‚´ ---
            # ROI ê°€ì´ë“œ ë¼ì¸ í‘œì‹œ
            cv2.line(display_frame, (roi_left, 0), (roi_left, h), (0, 0, 255), 2)
            cv2.line(display_frame, (roi_right, 0), (roi_right, h), (0, 0, 255), 2)

            current_entities = set() # (label, dist_bin, pos_bin)
            if closest_obj and min_meters < 10.0:
                b = closest_obj['box']
                label_name = closest_obj['label']
                meters = closest_obj['meters']
                
                # Generate Spatial Composite Key
                dist_bin = int(meters / self.DIST_BIN_SIZE)
                pos_bin = int((closest_obj['cx'] / w) / self.POS_BIN_SIZE)
                entity_key = (label_name, dist_bin, pos_bin)
                
                current_entities.add(entity_key)

                # ì‹œê°í™” (ì„ íƒëœ ë¬¼ì²´ë§Œ ê°•ì¡°)
                cv2.rectangle(display_frame, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 3)
                cv2.putText(display_frame, f"TARGET: {label_name} {meters:.1f}m", (b[0], b[1]-10), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

                # --- Rapid Approach Detection (Independent of Cooldown) ---
                if entity_key in self.entity_velocity_history:
                    last_dist, last_time = self.entity_velocity_history[entity_key]
                    dt = current_time - last_time
                    if dt > 0:
                        approach_speed = (last_dist - meters) / dt
                        if meters < self.APPROACH_THRESHOLD_DIST and approach_speed >= self.APPROACH_THRESHOLD_SPEED:
                            warning_msg = f"ìœ„í—˜! {label_name}ì´ ë§¤ìš° ë¹ ë¥´ê²Œ ì ‘ê·¼ ì¤‘ì…ë‹ˆë‹¤."
                            print(f"[RapidApproach] Speed: {approach_speed:.2f} m/s, Dist: {meters:.1f}m. Triggering Warning.")
                            self.speak(warning_msg, force_stop=True)
                
                self.entity_velocity_history[entity_key] = (meters, current_time)

                # --- ìŒì„± ì•ˆë‚´ ë¡œì§ (ë¼ë²¨ ì¿¨ë‹¤ìš´ ì²´í¬) ---
                # ë¼ë²¨ë³„ ì¿¨ë‹¤ìš´ ì²´í¬
                can_announce = True
                if label_name in self.label_cooldown:
                    if (current_time - self.label_cooldown[label_name]) < self.cooldown_time:
                        can_announce = False  # ì¿¨ë‹¤ìš´ ì¤‘

                if can_announce:
                    # Mark as announced immediately to prevent duplicate triggers
                    self.announced_objects[entity_key] = current_time
                    self.label_cooldown[label_name] = current_time  # ì¿¨ë‹¤ìš´ ì‹œì‘

                    # Determine position description
                    pos_desc = "ì •ë©´"
                    if closest_obj['cx'] < roi_left + (roi_right - roi_left) * 0.3:
                        pos_desc = "ì™¼ìª½"
                    elif closest_obj['cx'] > roi_left + (roi_right - roi_left) * 0.7:
                        pos_desc = "ì˜¤ë¥¸ìª½"

                    # ì¦‰ì‹œ ì•ˆë‚´ (ëª¨ë“  ê°ì²´)
                    immediate_msg = f"{pos_desc} {meters:.1f}ë¯¸í„°ì— {label_name}"
                    self.speak(immediate_msg)

                    # Trigger natural warning (LLM-based) with strict gating
                    self.follow_up_mgr.schedule_follow_up(label_name, meters, pos_desc, entity_key)

                if should_log:
                    print(f"[ë³´í–‰ ë³´ì¡°] ì¥ì• ë¬¼ ê°ì§€: {label_name} | ê°œì²´ í‚¤: {entity_key} | ê±°ë¦¬: {meters:.1f}m")

            # ì•ˆë‚´ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì˜¤ë«ë™ì•ˆ ì•ˆ ë³´ì¸ ì‚¬ë¬¼ì€ ëª©ë¡ì—ì„œ ì œê±°)
            for entity_key in list(self.announced_objects.keys()):
                if entity_key not in current_entities:
                    label_name = entity_key[0] # tuple (label, dist, pos)
                    # ê°ì§€ ì˜ì—­ì—ì„œ ì‚¬ë¼ì§ -> ì•ˆë‚´ ëª©ë¡ì—ì„œ ì‚­ì œ
                    if current_time - self.announced_objects[entity_key] > self.announce_timeout:
                        del self.announced_objects[entity_key]
                        # Also clean up velocity history
                        if entity_key in self.entity_velocity_history:
                            del self.entity_velocity_history[entity_key]
                        # ë§Œì•½ ì‚¬ë¼ì§„ ë¬¼ì²´ì— ëŒ€í•œ í›„ì† ì•ˆë‚´ê°€ ì˜ˆì•½ë˜ì–´ ìˆë‹¤ë©´ ì·¨ì†Œ
                        self.follow_up_mgr.cancel_pending()

            if should_log:
                last_log_time = current_time

            # ì›¹ ìŠ¤íŠ¸ë¦¬ë°ìš©ìœ¼ë¡œ í˜„ì¬ í”„ë ˆì„ ì €ì¥
            with self.frame_lock:
                self.last_web_frame = display_frame.copy()

            # GUI ëª¨ë“œì¼ ë•Œë§Œ í™”ë©´ í‘œì‹œ
            if not headless:
                cv2.imshow(window_name_main, display_frame)
                if self.last_depth_viz is not None:
                    cv2.imshow(window_name_depth, self.last_depth_viz)

                # ì¢…ë£Œ ë¡œì§
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q') or key == 27: # Që‚˜ ESC
                    break

                # ì°½ì´ ë‹«í˜”ëŠ”ì§€ í™•ì¸
                if cv2.getWindowProperty(window_name_main, cv2.WND_PROP_VISIBLE) < 1:
                    break
            else:
                # headless ëª¨ë“œì—ì„œëŠ” CPU ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ì§§ì€ ëŒ€ê¸°
                time.sleep(0.03)

        cap.release()
        if not headless:
            cv2.destroyAllWindows()

if __name__ == "__main__":
    pipeline = MVPTestPipeline()
    pipeline.run()
