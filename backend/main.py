"""
ì‹œê°ì¥ì• ì¸ì„ ìœ„í•œ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì§ˆë¬¸-ì‘ë‹µ API
Qwen2-VL-2B (ì˜ì–´) + M2M100 ë²ˆì—­ (í•œêµ­ì–´)
ì§€ì›: ì´ë¯¸ì§€ + ë¹„ë””ì˜¤ (10ì´ˆ ì´ë‚´ ê¶Œì¥)
"""

import io
import base64
import re
import asyncio
import gc
import tempfile
import os
from contextlib import asynccontextmanager
from typing import Optional, AsyncGenerator

import torch
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from PIL import Image

# ì „ì—­ ë³€ìˆ˜
vl_model = None
vl_processor = None
translator = None
translator_tokenizer = None
device = None

# ëª¨ë¸ ì„¤ì •
VL_MODEL_ID = "Qwen/Qwen2-VL-2B-Instruct"
TRANSLATOR_MODEL_ID = "facebook/m2m100_418M"

# ë¹„ë””ì˜¤ ì„¤ì •
MAX_VIDEO_DURATION = 10  # ìµœëŒ€ 10ì´ˆ
VIDEO_FPS = 1.0  # 1ì´ˆë‹¹ 1í”„ë ˆì„ ì¶”ì¶œ


# =============================================
# ì§ˆë¬¸ ë§¤í•‘ (ìš°ì„ ìˆœìœ„: ê¸´ íŒ¨í„´ë¶€í„°)
# =============================================

SUBJECT_MAP = {
    "ì‚°": "the mountain", "ë°”ë‹¤": "the ocean", "í˜¸ìˆ˜": "the lake",
    "ê°•": "the river", "í•˜ëŠ˜": "the sky", "êµ¬ë¦„": "the clouds",
    "ë‚˜ë¬´": "the trees", "ìˆ²": "the forest", "ê½ƒ": "the flowers",
    "í•´": "the sun", "ë‹¬": "the moon", "ë³„": "the stars",
    "ëˆˆ": "the snow", "ë¹„": "the rain", "ì•ˆê°œ": "the fog",
    "ë…¸ì„": "the sunset", "ì¼ì¶œ": "the sunrise",
    "ì–¸ë•": "the hill", "ì ˆë²½": "the cliff", "í•´ë³€": "the beach",
    "ë°”ìœ„": "the rocks", "í­í¬": "the waterfall", "ë“¤íŒ": "the field",
    "ê±´ë¬¼": "the building", "ì§‘": "the house", "ë‹¤ë¦¬": "the bridge",
    "ê¸¸": "the road", "ë°°": "the boat", "ë“±ëŒ€": "the lighthouse",
    "ì‚¬ëŒ": "the person", "ê³ ì–‘ì´": "the cat", "ê°•ì•„ì§€": "the dog",
    "ê°œ": "the dog", "ìƒˆ": "the bird", "ì°¨": "the car",
}

QUESTION_MAP = [
    ("ë­ê°€ ìˆì–´", "Describe this scene."),
    ("ë­ê°€ ë³´ì—¬", "Describe this scene."),
    ("ë­ ìˆì–´", "Describe this scene."),
    ("ì„¤ëª…í•´", "Describe this scene in detail."),
    ("ë­ í•´", "What is happening?"),
    ("ë­í•´", "What is happening?"),
    ("ë­í•˜ê³  ìˆì–´", "What is happening?"),
    ("ë¬´ìŠ¨ ì¼", "What is happening?"),
    ("ì›€ì§", "What is moving?"),
    ("í•˜ëŠ˜", "Describe the sky."),
    ("ë‚ ì”¨", "What is the weather like?"),
    ("ë¶„ìœ„ê¸°", "What is the mood or atmosphere?"),
    ("ë¬´ìŠ¨ ìƒ‰", "What colors do you see?"),
    ("ìƒ‰ê¹”", "What are the main colors?"),
    ("ëª‡ ëª…", "How many people are there?"),
    ("ëª‡ ë§ˆë¦¬", "How many animals are there?"),
    ("ì–´ë””", "What place is this?"),
    ("ì–´ë•Œ", "How does this look?"),
]


def convert_question_to_english(question: str) -> str:
    """í•œêµ­ì–´ ì§ˆë¬¸ì„ ì˜ì–´ë¡œ ë³€í™˜"""
    subject = "it"
    found_subject_ko = None
    for ko, en in SUBJECT_MAP.items():
        if ko in question:
            subject = en
            found_subject_ko = ko
            break
    
    for pattern, en_template in QUESTION_MAP:
        if pattern in question:
            return en_template.replace("{subject}", subject)
    
    if found_subject_ko:
        return f"Describe {subject}."
    
    return "Describe this scene."


# =============================================
# ë²ˆì—­ í›„ì²˜ë¦¬
# =============================================

def clean_translation(text: str) -> str:
    """ë²ˆì—­ ê²°ê³¼ ì •ë¦¬"""
    if not text:
        return ""
    
    # ë§ˆí¬ë‹¤ìš´ ì œê±°
    text = re.sub(r'#{1,6}\s*', '', text)
    text = re.sub(r'\*{1,2}([^*]+)\*{1,2}', r'\1', text)
    text = re.sub(r'^\s*[-*]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*\d+\.\s+', '', text, flags=re.MULTILINE)
    
    # ë¶ˆí•„ìš”í•œ í‘œí˜„ ì œê±°
    remove_patterns = [
        r'ì´ë¯¸ì§€ì—ì„œ\s*', r'ì‚¬ì§„ì—ì„œ\s*', r'ê·¸ë¦¼ì€\s*',
        r'ë¹„ë””ì˜¤ì—ì„œ\s*', r'ì˜ìƒì—ì„œ\s*',
        r'ê·¸ê²ƒì€\s*', r'ì´ê²ƒì€\s*',
        r'\(.*?\)', r'í”„ë ˆì„ì˜\s*',
    ]
    for pattern in remove_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # ë²ˆì—­ ì˜¤ë¥˜ ìˆ˜ì •
    replacements = [
        ('ìˆìŠµë‹ˆë‹¤.', 'ìˆë„¤ìš”.'), ('ë³´ì…ë‹ˆë‹¤.', 'ë³´ì´ë„¤ìš”.'),
        ('ì…ë‹ˆë‹¤.', 'ì´ì—ìš”.'), ('í•©ë‹ˆë‹¤.', 'í•´ìš”.'),
        ('í‘œì§€íŒ', 'ë¬´ëŠ¬'), ('ëƒ„ë¹„', 'í„¸'), ('ë²½ëŒ', 'ë¬´ëŠ¬'),
        ('ë‘ ê°œì˜ ê³ ì–‘ì´', 'ê³ ì–‘ì´ ë‘ ë§ˆë¦¬'),
        ('ë‘ ê°œì˜', 'ë‘ '),
    ]
    for old, new in replacements:
        text = text.replace(old, new)
    
    # ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    text = text.replace(';', '.')
    
    # 2ë¬¸ì¥ ì œí•œ
    sentences = re.split(r'(?<=[.!?ìš”])\s+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    if len(sentences) > 2:
        text = ' '.join(sentences[:2])
    
    # ë¬¸ì¥ ë ì²˜ë¦¬
    if text and not text[-1] in '.!?ìš”':
        text += 'ìš”.'
    
    return text.strip()


# =============================================
# ëª¨ë¸ ë¡œë“œ
# =============================================

def load_models():
    """ëª¨ë¸ ë¡œë“œ"""
    global vl_model, vl_processor, translator, translator_tokenizer, device
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ Using device: {device}")
    
    if device == "cuda":
        torch.cuda.empty_cache()
        gc.collect()
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
    
    try:
        # VL ëª¨ë¸
        print(f"ğŸ“¦ Loading VL model: {VL_MODEL_ID}")
        from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
        
        vl_processor = AutoProcessor.from_pretrained(
            VL_MODEL_ID, trust_remote_code=True,
            min_pixels=256*28*28, max_pixels=512*28*28,
        )
        vl_model = Qwen2VLForConditionalGeneration.from_pretrained(
            VL_MODEL_ID,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True,
        )
        print("âœ… VL model loaded!")
        
        # ë²ˆì—­ ëª¨ë¸
        print(f"ğŸ“¦ Loading translator: {TRANSLATOR_MODEL_ID}")
        from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer
        
        translator_tokenizer = M2M100Tokenizer.from_pretrained(TRANSLATOR_MODEL_ID)
        translator = M2M100ForConditionalGeneration.from_pretrained(TRANSLATOR_MODEL_ID)
        if device == "cuda":
            translator = translator.to(device)
        print("âœ… Translator loaded!")
        print("ğŸš€ All models ready!")
        
    except Exception as e:
        print(f"âŒ Model loading failed: {e}")
        raise e


def translate_to_korean(text: str) -> str:
    """ì˜ì–´ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
    global translator, translator_tokenizer, device
    
    if not text.strip():
        return ""
    
    try:
        translator_tokenizer.src_lang = "en"
        inputs = translator_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
        if device == "cuda":
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            generated_tokens = translator.generate(
                **inputs,
                forced_bos_token_id=translator_tokenizer.get_lang_id("ko"),
                max_length=128, num_beams=3,
            )
        
        translated = translator_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        return clean_translation(translated)
    except Exception as e:
        print(f"Translation error: {e}")
        return text


# =============================================
# FastAPI
# =============================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    load_models()
    yield
    global vl_model, vl_processor, translator, translator_tokenizer
    del vl_model, vl_processor, translator, translator_tokenizer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()


app = FastAPI(title="Vision Assistant API", version="3.0.0", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])


class QuestionRequest(BaseModel):
    image_base64: str
    question: str
    language: str = "ko"


class VideoQuestionRequest(BaseModel):
    video_base64: str
    question: str
    language: str = "ko"


class AnswerResponse(BaseModel):
    answer: str
    success: bool
    error: Optional[str] = None


def process_image(image_data: str) -> Image.Image:
    """Base64 ì´ë¯¸ì§€ ë””ì½”ë”©"""
    if "," in image_data:
        image_data = image_data.split(",")[1]
    image_bytes = base64.b64decode(image_data)
    return Image.open(io.BytesIO(image_bytes)).convert("RGB")


def process_video(video_data: str) -> str:
    """Base64 ë¹„ë””ì˜¤ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥"""
    if "," in video_data:
        video_data = video_data.split(",")[1]
    video_bytes = base64.b64decode(video_data)
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as f:
        f.write(video_bytes)
        return f.name


# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
IMAGE_PROMPT = """Say what you see in ONE simple sentence. 10 words maximum.
Example: "A cat and dog sitting on grass."
Do NOT describe colors, positions, shadows, or background."""

VIDEO_PROMPT = """Describe what is happening in this video in 1-2 simple sentences.
Focus on the main action or movement. 20 words maximum."""


def clean_english_answer(text: str) -> str:
    """ì˜ì–´ ë‹µë³€ ì •ë¦¬"""
    if not text:
        return ""
    
    text = re.sub(r'\([^)]*\)', '', text)
    remove_phrases = [
        r'which\s+.*?[,.]', r'with\s+shadows?\s+.*?[,.]',
        r'in\s+the\s+background.*?[,.]', r'off[\s-]camera.*?[,.]',
    ]
    for pattern in remove_phrases:
        text = re.sub(pattern, '.', text, flags=re.IGNORECASE)
    
    sentences = re.split(r'[.!?;]', text)
    if sentences:
        text = sentences[0].strip()
    
    if text and not text.endswith('.'):
        text += '.'
    
    return text.strip()


def generate_english_answer(image: Image.Image, question: str) -> str:
    """ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±"""
    global vl_model, vl_processor, device
    from qwen_vl_utils import process_vision_info
    
    messages = [
        {"role": "system", "content": IMAGE_PROMPT},
        {"role": "user", "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": question},
        ]}
    ]
    
    text = vl_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = vl_processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt")
    
    if device == "cuda":
        inputs = inputs.to("cuda")
    
    with torch.no_grad():
        generated_ids = vl_model.generate(**inputs, max_new_tokens=30, do_sample=False, repetition_penalty=1.5)
    
    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
    answer = vl_processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    return answer.strip()


def generate_english_answer_video(video_path: str, question: str) -> str:
    """ë¹„ë””ì˜¤ ì„¤ëª… ìƒì„±"""
    global vl_model, vl_processor, device
    from qwen_vl_utils import process_vision_info
    
    messages = [
        {"role": "system", "content": VIDEO_PROMPT},
        {"role": "user", "content": [
            {"type": "video", "video": video_path, "max_pixels": 360 * 420, "fps": VIDEO_FPS},
            {"type": "text", "text": question},
        ]}
    ]
    
    text = vl_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = vl_processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt")
    
    if device == "cuda":
        inputs = inputs.to("cuda")
    
    with torch.no_grad():
        generated_ids = vl_model.generate(**inputs, max_new_tokens=50, do_sample=False, repetition_penalty=1.5)
    
    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]
    answer = vl_processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    return answer.strip()


def generate_answer(image: Image.Image, question: str, language: str = "ko") -> str:
    """ì´ë¯¸ì§€ ë‹µë³€ ìƒì„±"""
    if language == "ko":
        en_question = convert_question_to_english(question)
        print(f"ğŸ”„ ì§ˆë¬¸: '{question}' â†’ '{en_question}'")
    else:
        en_question = question
    
    english_answer = generate_english_answer(image, en_question)
    print(f"ğŸ‡ºğŸ‡¸ ì˜ì–´: {english_answer}")
    
    english_answer = clean_english_answer(english_answer)
    
    if language == "ko":
        korean_answer = translate_to_korean(english_answer)
        print(f"ğŸ‡°ğŸ‡· í•œêµ­ì–´: {korean_answer}")
        return korean_answer
    return english_answer


def generate_answer_video(video_path: str, question: str, language: str = "ko") -> str:
    """ë¹„ë””ì˜¤ ë‹µë³€ ìƒì„±"""
    if language == "ko":
        en_question = convert_question_to_english(question)
        print(f"ğŸ”„ ì§ˆë¬¸: '{question}' â†’ '{en_question}'")
    else:
        en_question = question
    
    english_answer = generate_english_answer_video(video_path, en_question)
    print(f"ğŸ‡ºğŸ‡¸ ì˜ì–´: {english_answer}")
    
    english_answer = clean_english_answer(english_answer)
    
    if language == "ko":
        korean_answer = translate_to_korean(english_answer)
        print(f"ğŸ‡°ğŸ‡· í•œêµ­ì–´: {korean_answer}")
        return korean_answer
    return english_answer


async def generate_stream(image: Image.Image, question: str, language: str = "ko") -> AsyncGenerator[str, None]:
    """ì´ë¯¸ì§€ ìŠ¤íŠ¸ë¦¬ë°"""
    if language == "ko":
        en_question = convert_question_to_english(question)
    else:
        en_question = question
    
    english_answer = generate_english_answer(image, en_question)
    english_answer = clean_english_answer(english_answer)
    final_answer = translate_to_korean(english_answer) if language == "ko" else english_answer
    
    for char in final_answer:
        yield char
        await asyncio.sleep(0.02)


async def generate_stream_video(video_path: str, question: str, language: str = "ko") -> AsyncGenerator[str, None]:
    """ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë°"""
    if language == "ko":
        en_question = convert_question_to_english(question)
    else:
        en_question = question
    
    english_answer = generate_english_answer_video(video_path, en_question)
    english_answer = clean_english_answer(english_answer)
    final_answer = translate_to_korean(english_answer) if language == "ko" else english_answer
    
    for char in final_answer:
        yield char
        await asyncio.sleep(0.02)


# =============================================
# API ì—”ë“œí¬ì¸íŠ¸
# =============================================

@app.post("/api/ask", response_model=AnswerResponse)
async def ask_question(request: QuestionRequest):
    try:
        image = process_image(request.image_base64)
        answer = generate_answer(image, request.question, request.language)
        return AnswerResponse(answer=answer, success=True)
    except Exception as e:
        print(f"âŒ Error: {e}")
        return AnswerResponse(answer="ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.", success=False, error=str(e))


@app.post("/api/ask-stream")
async def ask_question_stream(request: QuestionRequest):
    try:
        image = process_image(request.image_base64)
        
        async def event_generator():
            async for chunk in generate_stream(image, request.question, request.language):
                yield f"data: {chunk}\n\n"
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(event_generator(), media_type="text/event-stream",
            headers={"Cache-Control": "no-cache", "Connection": "keep-alive"})
    except Exception as e:
        return AnswerResponse(answer="ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.", success=False, error=str(e))


@app.post("/api/describe-stream")
async def describe_image_stream(request: QuestionRequest):
    try:
        image = process_image(request.image_base64)
        
        async def event_generator():
            async for chunk in generate_stream(image, "Describe this scene.", request.language):
                yield f"data: {chunk}\n\n"
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(event_generator(), media_type="text/event-stream",
            headers={"Cache-Control": "no-cache", "Connection": "keep-alive"})
    except Exception as e:
        return AnswerResponse(answer="ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.", success=False, error=str(e))


# ë¹„ë””ì˜¤ API

@app.post("/api/ask-video", response_model=AnswerResponse)
async def ask_video_question(request: VideoQuestionRequest):
    video_path = None
    try:
        video_path = process_video(request.video_base64)
        answer = generate_answer_video(video_path, request.question, request.language)
        return AnswerResponse(answer=answer, success=True)
    except Exception as e:
        print(f"âŒ Error: {e}")
        return AnswerResponse(answer="ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.", success=False, error=str(e))
    finally:
        if video_path and os.path.exists(video_path):
            os.remove(video_path)


@app.post("/api/ask-video-stream")
async def ask_video_question_stream(request: VideoQuestionRequest):
    video_path = None
    try:
        video_path = process_video(request.video_base64)
        
        async def event_generator():
            async for chunk in generate_stream_video(video_path, request.question, request.language):
                yield f"data: {chunk}\n\n"
            yield "data: [DONE]\n\n"
            if video_path and os.path.exists(video_path):
                os.remove(video_path)
        
        return StreamingResponse(event_generator(), media_type="text/event-stream",
            headers={"Cache-Control": "no-cache", "Connection": "keep-alive"})
    except Exception as e:
        if video_path and os.path.exists(video_path):
            os.remove(video_path)
        return AnswerResponse(answer="ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.", success=False, error=str(e))


@app.post("/api/describe-video-stream")
async def describe_video_stream(request: VideoQuestionRequest):
    video_path = None
    try:
        video_path = process_video(request.video_base64)
        
        async def event_generator():
            async for chunk in generate_stream_video(video_path, "What is happening in this video?", request.language):
                yield f"data: {chunk}\n\n"
            yield "data: [DONE]\n\n"
            if video_path and os.path.exists(video_path):
                os.remove(video_path)
        
        return StreamingResponse(event_generator(), media_type="text/event-stream",
            headers={"Cache-Control": "no-cache", "Connection": "keep-alive"})
    except Exception as e:
        if video_path and os.path.exists(video_path):
            os.remove(video_path)
        return AnswerResponse(answer="ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”.", success=False, error=str(e))


# =============================================
# ì •ì  íŒŒì¼ ì„œë¹™ (cloudflared ë°°í¬ìš©)
# =============================================

# í”„ë¡ íŠ¸ì—”ë“œ ë¹Œë“œ íŒŒì¼ ê²½ë¡œ
frontend_dist = os.path.join(os.path.dirname(__file__), "..", "frontend", "dist")

if os.path.exists(frontend_dist):
    # assets ë“± ì •ì  íŒŒì¼ ë§ˆìš´íŠ¸
    app.mount("/assets", StaticFiles(directory=os.path.join(frontend_dist, "assets")), name="static")

    # SPAë¥¼ ìœ„í•œ catch-all ë¼ìš°íŠ¸ (ëª¨ë“  ê²½ë¡œë¥¼ index.htmlë¡œ)
    @app.get("/{full_path:path}")
    async def serve_spa(full_path: str):
        # API ê²½ë¡œê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ index.html ì„œë¹™
        if full_path.startswith("api/"):
            return JSONResponse({"error": "Not found"}, status_code=404)

        index_path = os.path.join(frontend_dist, "index.html")
        if os.path.exists(index_path):
            return FileResponse(index_path)
        return JSONResponse({"error": "Frontend not built"}, status_code=404)
else:
    # í”„ë¡ íŠ¸ì—”ë“œ ë¹Œë“œ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ë¡œ ì•ˆë‚´
    @app.get("/")
    async def root():
        return {
            "status": "running", "version": "3.0.0",
            "vl_model": VL_MODEL_ID, "translator": TRANSLATOR_MODEL_ID,
            "cuda_available": torch.cuda.is_available(),
            "features": ["ì´ë¯¸ì§€ ì¸ì‹", "ë¹„ë””ì˜¤ ì¸ì‹", "ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"],
            "note": "Frontend not built. Build frontend first for web access."
        }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)